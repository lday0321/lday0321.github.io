<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>lday的博客</title>
  <subtitle>Life, Coding, Funning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://lday.me/"/>
  <updated>2020-12-06T10:40:17.607Z</updated>
  <id>http://lday.me/</id>
  
  <author>
    <name>lday</name>
    <email>oneday0321@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TEST</title>
    <link href="http://lday.me/2020/12/06/0027_test/"/>
    <id>http://lday.me/2020/12/06/0027_test/</id>
    <published>2020-12-06T13:56:14.000Z</published>
    <updated>2020-12-06T10:40:17.607Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;test!!!&lt;/p&gt;

    
    </summary>
    
    
      <category term="Aerospike" scheme="http://lday.me/tags/Aerospike/"/>
    
      <category term="NoSQL" scheme="http://lday.me/tags/NoSQL/"/>
    
      <category term="Memory" scheme="http://lday.me/tags/Memory/"/>
    
      <category term="Jemalloc" scheme="http://lday.me/tags/Jemalloc/"/>
    
  </entry>
  
  <entry>
    <title>Aerospike的内存管理</title>
    <link href="http://lday.me/2020/03/07/0026_aerospike_memory_management/"/>
    <id>http://lday.me/2020/03/07/0026_aerospike_memory_management/</id>
    <published>2020-03-07T13:56:14.000Z</published>
    <updated>2020-03-07T09:14:07.694Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/aerospike&quot;&gt;Aerospike&lt;/a&gt;使用&lt;a href=&quot;http://jemalloc.net&quot;&gt;jemalloc&lt;/a&gt;进行内存分配和管理。利用了&lt;a href=&quot;http://jemalloc.net&quot;&gt;jemalloc&lt;/a&gt;提供的扩展特性来更细力度的进行内存控制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Aerospike" scheme="http://lday.me/tags/Aerospike/"/>
    
      <category term="NoSQL" scheme="http://lday.me/tags/NoSQL/"/>
    
      <category term="Memory" scheme="http://lday.me/tags/Memory/"/>
    
      <category term="Jemalloc" scheme="http://lday.me/tags/Jemalloc/"/>
    
  </entry>
  
  <entry>
    <title>Aerospike内部逻辑：从Record Put说起</title>
    <link href="http://lday.me/2020/02/15/0025_aerospike_internal_put_key/"/>
    <id>http://lday.me/2020/02/15/0025_aerospike_internal_put_key/</id>
    <published>2020-02-15T13:56:14.000Z</published>
    <updated>2020-03-07T08:59:34.764Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/aerospike&quot;&gt;Aerospike&lt;/a&gt;是一个分布式的Key-Value存储系统，广泛应用于广告行业的个性化推荐及实时竞价系统中。其主要特点是采用混合存储架构，数据索引信息存储在内存中，而数据本身则存储在硬盘上。Aerospike可绕过文件系统，独立管理写入磁盘的数据，以裸盘访问形式直接读写硬盘，从而达到高效的数据访问性能。本文以数据写入(Record Put)为例，来一窥Aerospike的内部逻辑结构(Aerospike参考版本：4.5.3.3)。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Aerospike" scheme="http://lday.me/tags/Aerospike/"/>
    
      <category term="NoSQL" scheme="http://lday.me/tags/NoSQL/"/>
    
  </entry>
  
  <entry>
    <title>HDFS Lease(租约)逻辑</title>
    <link href="http://lday.me/2020/01/28/0024_hdfs_lease_internal/"/>
    <id>http://lday.me/2020/01/28/0024_hdfs_lease_internal/</id>
    <published>2020-01-28T15:55:14.000Z</published>
    <updated>2020-01-28T15:35:29.355Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;HDFS对于同一个文件支持一写多读（write-once-read-many）。为了保持数据一致性，当一个客户端往HDFS某个文件写数据时， 其他客户端不允许同时写入。HDFS引入Lease(租约)机制来实现“独写”控制。 &lt;/p&gt;
&lt;p&gt;本文基于hadoop-2.7.2版本对HDFS的租约机制进行整理分析&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://lday.me/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://lday.me/tags/HDFS/"/>
    
      <category term="lease" scheme="http://lday.me/tags/lease/"/>
    
  </entry>
  
  <entry>
    <title>Linux内核Page Cache和Buffer Cache关系及演化历史</title>
    <link href="http://lday.me/2019/09/09/0023_linux_page_cache_and_buffer_cache/"/>
    <id>http://lday.me/2019/09/09/0023_linux_page_cache_and_buffer_cache/</id>
    <published>2019-09-08T16:55:14.000Z</published>
    <updated>2019-09-08T17:22:10.145Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在我们进行数据持久化，对文件内容进行落盘处理时，我们时常会使用fsync操作，该操作会将文件关联的脏页(dirty page)数据(实际文件内容及元数据信息)一同写回磁盘。这里提到的脏页(dirty page)即为页缓存(page cache)。&lt;/p&gt;
&lt;p&gt;块缓存(buffer cache)，则是内核为了加速对底层存储介质的访问速度，而构建的一层缓存。他缓存部分磁盘数据，当有磁盘读取请求时，会首先查看块缓存中是否有对应的数据，如果有的话，则直接将对应数据返回，从而减少对磁盘的访问。&lt;/p&gt;
&lt;p&gt;两层缓存各有自己的缓存目标，我好奇的是，这两者到底是什么关系。本文主要参考若干kernel资料，对应的kernel源码版本主要包括：linux-0.11, linux-2.2.16, linux-2.4.0, linux-2.4.19, linux-2.6.18。&lt;/p&gt;
    
    </summary>
    
    
      <category term="linux" scheme="http://lday.me/tags/linux/"/>
    
      <category term="kernel" scheme="http://lday.me/tags/kernel/"/>
    
      <category term="storage" scheme="http://lday.me/tags/storage/"/>
    
      <category term="cache" scheme="http://lday.me/tags/cache/"/>
    
  </entry>
  
  <entry>
    <title>如何实现分布式锁</title>
    <link href="http://lday.me/2018/11/18/0022_how_to_do_distributed_lock/"/>
    <id>http://lday.me/2018/11/18/0022_how_to_do_distributed_lock/</id>
    <published>2018-11-18T12:58:14.000Z</published>
    <updated>2018-12-02T07:07:42.855Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;分布式锁服务在分布式系统中是一个非常通用的需求。互联网行业有基于Zookeeper实现分布式锁服务的方案，也有提出基于Redis实现分布式锁服务的方案。企业级应用方面，开源Linux上，Redhat Linux HA套件中提供了&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/high_availability_add-on_overview/ch-dlm&quot;&gt;DLM(Distributed Lock Manager)&lt;/a&gt;，商用操作系统OpenVMS也提供了&lt;a href=&quot;http://neilrieck.net/docs/openvms_notes_DLM.html&quot;&gt;DLM&lt;/a&gt;。下面的这篇文章，是我于18年初的时候读到的，最近又重读一遍，作为回顾总结。翻译自Martin Kleppmann的博文：&lt;a href=&quot;https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html&quot;&gt;How to do distributed locking&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="分布式" scheme="http://lday.me/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="redis" scheme="http://lday.me/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kafka的分布式系统消防员(Controller Broker)</title>
    <link href="http://lday.me/2018/11/06/0021_apache_kafka_firefighter/"/>
    <id>http://lday.me/2018/11/06/0021_apache_kafka_firefighter/</id>
    <published>2018-11-06T14:52:14.000Z</published>
    <updated>2018-11-18T06:50:17.673Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;作为分布式系统的Kafka，在管理、协调分布式节点，处理各类分布式系统事件时，都依赖于Controller Broker来完成。Controller Broker的工作包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Broker节点上、下线时，处理Broker节点的故障转移。&lt;/li&gt;
&lt;li&gt;Topic新建或删除时，Partition扩容时，处理Partition的分配。&lt;/li&gt;
&lt;li&gt;管理所有Partition的状态机，以及所有Replica的状态机，处理状态机的变化。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以下内容介绍了Controller Broker的具体功能、逻辑，它翻译自：&lt;a href=&quot;https://hackernoon.com/apache-kafkas-distributed-system-firefighter-the-controller-broker-1afca1eae302&quot;&gt;Apache Kafka’s Distributed System Firefighter&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="kafka" scheme="http://lday.me/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>B-Tree数据结构总结</title>
    <link href="http://lday.me/2018/02/21/0020_b_tree_summary/"/>
    <id>http://lday.me/2018/02/21/0020_b_tree_summary/</id>
    <published>2018-02-21T06:04:14.000Z</published>
    <updated>2018-02-24T15:09:11.737Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;对B树的理解一直停留在概念阶段，一直也想了解一些细节。趁着过年这段时间，把《算法导论》关于B树的章节（第18章）整个过了一遍。结合书上的说明，自己实现了一把，留为总结。以下文字内容多摘自《算法导论》，伪代码部分根据我自己的理解添加了注释。&lt;/p&gt;
    
    </summary>
    
    
      <category term="cpp" scheme="http://lday.me/tags/cpp/"/>
    
      <category term="ds_algorithm" scheme="http://lday.me/tags/ds-algorithm/"/>
    
      <category term="b-tree" scheme="http://lday.me/tags/b-tree/"/>
    
  </entry>
  
  <entry>
    <title>共享内存(ipc)的使用</title>
    <link href="http://lday.me/2017/12/17/0019_boost_share_memory_ipc/"/>
    <id>http://lday.me/2017/12/17/0019_boost_share_memory_ipc/</id>
    <published>2017-12-17T15:08:14.000Z</published>
    <updated>2017-12-17T15:53:47.943Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;共享内存，是进程间数据传输的方式之一。数据发送方将数据放入共享内存中，数据接收方则从共享内存中将数据读出，进而完成整个数据的传输。这里我通过使用场景的方式简单总结&lt;code&gt;boost::interprocess share_memory&lt;/code&gt;的使用&lt;/p&gt;
    
    </summary>
    
    
      <category term="cpp" scheme="http://lday.me/tags/cpp/"/>
    
      <category term="boost" scheme="http://lday.me/tags/boost/"/>
    
      <category term="share memory" scheme="http://lday.me/tags/share-memory/"/>
    
      <category term="ipc" scheme="http://lday.me/tags/ipc/"/>
    
  </entry>
  
  <entry>
    <title>C++内存屏障（内存顺序）总结</title>
    <link href="http://lday.me/2017/12/02/0018_cpp_atomic_summary/"/>
    <id>http://lday.me/2017/12/02/0018_cpp_atomic_summary/</id>
    <published>2017-12-02T07:09:14.000Z</published>
    <updated>2019-01-24T15:25:16.540Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;原子操作（atomic）是无锁编程(Lock-Free Programming)的基础。以往，要使用atomic操作，我们一般会使用&lt;a href=&quot;https://gcc.gnu.org/onlinedocs/gcc-4.9.2/gcc/_005f_005fatomic-Builtins.html&quot;&gt;gcc内置的原子操作接口&lt;/a&gt;，或者是基于指定平台硬件指令封装的&lt;a href=&quot;https://github.com/ivmai/libatomic_ops&quot;&gt;atomic库&lt;/a&gt;。c++11直接引入了&lt;a href=&quot;http://en.cppreference.com/w/cpp/atomic&quot;&gt;atomic库&lt;/a&gt;，为c++定义了原子类型操作接口以及内存模型，极大的方便了我们的使用。我尝试通过本文对C++11中内存屏障（内存顺序）的一些基本概念和使用情况进行总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="cpp" scheme="http://lday.me/tags/cpp/"/>
    
      <category term="concurrency" scheme="http://lday.me/tags/concurrency/"/>
    
      <category term="atomic" scheme="http://lday.me/tags/atomic/"/>
    
  </entry>
  
  <entry>
    <title>Linux下Condition Vairable和Mutext合用的小细节</title>
    <link href="http://lday.me/2017/11/19/0017_condition_variable_and_mutex_together/"/>
    <id>http://lday.me/2017/11/19/0017_condition_variable_and_mutex_together/</id>
    <published>2017-11-19T06:04:14.000Z</published>
    <updated>2017-11-19T06:34:51.815Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在Linux下合用Condition Variable和Mutex有些小细节。为什么需要这么做，总结于此。&lt;/p&gt;
    
    </summary>
    
    
      <category term="linux" scheme="http://lday.me/tags/linux/"/>
    
      <category term="concurrency" scheme="http://lday.me/tags/concurrency/"/>
    
  </entry>
  
  <entry>
    <title>什么是内存屏障(Memory Barriers)</title>
    <link href="http://lday.me/2017/11/04/0016_what_is_memory_barriers/"/>
    <id>http://lday.me/2017/11/04/0016_what_is_memory_barriers/</id>
    <published>2017-11-04T11:25:14.000Z</published>
    <updated>2019-01-24T15:06:14.487Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;内存屏障是一种底层原语，在不同计算机架构下有不同的实现细节。本文主要在x86_64处理器下，通过Linux及其内核代码来分析和使用内存屏障&lt;/p&gt;
&lt;p&gt;对大多数应用层开发者来说，“内存屏障”（memory barrier）是一种陌生，甚至有些诡异的技术。实际上，他常被用在操作系统内核中，用于实现同步机制、驱动程序等。利用它，能实现高效的无锁数据结构，提高多线程程序的性能表现。本文首先探讨了内存屏障的必要性，之后介绍如何使用内存屏障实现一个无锁唤醒缓冲区（队列），用于在多个线程间进行高效的数据交换。&lt;/p&gt;
    
    </summary>
    
    
      <category term="memory barriers" scheme="http://lday.me/tags/memory-barriers/"/>
    
  </entry>
  
  <entry>
    <title>我们如何在Go中使用gRPC构建C/S结构系统</title>
    <link href="http://lday.me/2017/10/22/0015_How_we_use_gRPC_to_build_a_client_server_system_in_Go/"/>
    <id>http://lday.me/2017/10/22/0015_How_we_use_gRPC_to_build_a_client_server_system_in_Go/</id>
    <published>2017-10-22T15:14:14.000Z</published>
    <updated>2017-10-22T15:37:39.994Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;RPC作为分布式系统节点中较为主流的一种通信方式，因其简单、方便的调用模式而深受开发者的喜爱。gRPC作为google推出的RPC调用框架，更是受到大家的关注。gRPC原生提供了对Golang的支持，最近，我也在关注gRPC的使用，本文是我整理的&lt;em&gt;&lt;a href=&quot;https://medium.com/pantomath/how-we-use-grpc-to-build-a-client-server-system-in-go-dd20045fa1c2&quot;&gt;《How we use gRPC to build a client/server system in Go》&lt;/a&gt;&lt;/em&gt;翻译:&lt;/p&gt;
    
    </summary>
    
    
      <category term="golang" scheme="http://lday.me/tags/golang/"/>
    
      <category term="gRPC" scheme="http://lday.me/tags/gRPC/"/>
    
  </entry>
  
  <entry>
    <title>Kafka数据丢失及最新改进策略</title>
    <link href="http://lday.me/2017/10/08/0014_kafka_data_loss_and_new_mechanism/"/>
    <id>http://lday.me/2017/10/08/0014_kafka_data_loss_and_new_mechanism/</id>
    <published>2017-10-08T14:46:14.000Z</published>
    <updated>2018-12-29T15:25:00.961Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;上周在测试环境，无意间连续把两台kafka的磁盘打爆，导致broker相继挂掉。当我清理完磁盘，重启两台broker后，发现很有意思的现象：&lt;strong&gt;kafka数据丢失！&lt;/strong&gt;从我们的处理日志上， 我们观察到有向一个topic的两个partition，分别写入了3条和7条，共计10条消息。但是，当我们恢复两台broker后，通过命令查看，发现这时该topic两个partition消息数量竟然都是0，而从zk上consumer group记录的信息来看，有cg的确已经消费过该topic的数据，且cg在对应partition上的offset分别为3,7。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kafka" scheme="http://lday.me/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>一次Golang程序延迟过大问题的定位过程</title>
    <link href="http://lday.me/2017/09/13/0013_a_latency_identification_procedure/"/>
    <id>http://lday.me/2017/09/13/0013_a_latency_identification_procedure/</id>
    <published>2017-09-13T14:27:14.000Z</published>
    <updated>2020-11-22T01:43:58.756Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;高吞吐、低延迟，一直是后台程序追求的目标。上周，我对PubSub系统进行了内存泄漏的分析定位，这周进一步对系统处理的延迟进行了分析，找到了引起处理延迟的关键节点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="golang" scheme="http://lday.me/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>一次Golang程序内存泄漏分析之旅</title>
    <link href="http://lday.me/2017/09/02/0012_a_memory_leak_detection_procedure/"/>
    <id>http://lday.me/2017/09/02/0012_a_memory_leak_detection_procedure/</id>
    <published>2017-09-01T18:49:14.000Z</published>
    <updated>2020-11-22T01:51:16.763Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;最近在开发升级PubSub系统，目标是支持更新版本的kafka（从原来的支持kafka 0.8.2.2升级到支持较新版本的kafka 0.10.1.1）。由于kafka在0.9版本上对consumer group相关的结构进行了&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Client-side+Assignment+Proposal&quot;&gt;重构&lt;/a&gt;，原来使用的基于zk进行rebalance的&lt;a href=&quot;https://github.com/wvanbergen/kafka&quot;&gt;consumer group client:wvanbergen/kafka&lt;/a&gt;已经不再适用。为此，我们调研并最终选用了支持新版kafka consumer rebalance的&lt;a href=&quot;https://github.com/bsm/sarama-cluster&quot;&gt;consumer group client:sarama-cluster&lt;/a&gt;。功能开发已基本结束，目前已进入系统压力测试阶段。&lt;/p&gt;
    
    </summary>
    
    
      <category term="golang" scheme="http://lday.me/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Consumer Rebalance的演进</title>
    <link href="http://lday.me/2017/07/24/0011_kafka_consumer_rebalance_evolution/"/>
    <id>http://lday.me/2017/07/24/0011_kafka_consumer_rebalance_evolution/</id>
    <published>2017-07-24T15:25:14.000Z</published>
    <updated>2017-07-24T15:25:11.742Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Kafka引入了Consumer Group的概念。在一个Consumer Group中的若干Consumer，会按照一定规则（均匀）分配同一topic下的所有partition，各自在相应partition上执行sub工作。当一个Group中有Consumer退出时，Group中的其他Consumer会对topic下的partition进行重新分配，并基于重新分配的结果继续执行sub工作。这个重新分配的过程被称为Rebalance。&lt;/p&gt;
&lt;p&gt;对于Sub方的应用而言，Rebalance过程是透明的，Consumer Group Rebalance实现也经历了几个版本的演进，本文对Rebalance的实现方案进行大致的梳理和总结&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kafka" scheme="http://lday.me/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Procuder小结</title>
    <link href="http://lday.me/2017/07/15/0010_kafka_producer_analysis_01/"/>
    <id>http://lday.me/2017/07/15/0010_kafka_producer_analysis_01/</id>
    <published>2017-07-15T06:05:05.000Z</published>
    <updated>2017-07-15T06:05:31.209Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;理解Kafka 0.10.0.1版本的Procuder代码，对Producer的整体逻辑进行小结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kafka" scheme="http://lday.me/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>为什么没有收到预期的413状态码</title>
    <link href="http://lday.me/2017/07/11/0009_why_not_413/"/>
    <id>http://lday.me/2017/07/11/0009_why_not_413/</id>
    <published>2017-07-11T13:21:21.000Z</published>
    <updated>2017-07-11T13:33:00.443Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;HTTP状态码413的含义是请求实体过长，RFC7231对413状态码的定义如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;6.5.11.  413 Payload Too Large&lt;/strong&gt;&lt;br&gt;The 413 (Payload Too Large) status code indicates that the server is refusing to process a request because the request payload is larger than the server is willing or able to process.  The server MAY close the connection to prevent the client from continuing the request.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="golang" scheme="http://lday.me/tags/golang/"/>
    
      <category term="HTTP" scheme="http://lday.me/tags/HTTP/"/>
    
      <category term="java" scheme="http://lday.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Kafka与传统消息中间件的差异</title>
    <link href="http://lday.me/2017/06/27/0008_kafka_vs_tranditional_mq/"/>
    <id>http://lday.me/2017/06/27/0008_kafka_vs_tranditional_mq/</id>
    <published>2017-06-27T14:21:21.000Z</published>
    <updated>2017-06-27T14:21:59.839Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;因工作关系，之前稍微接触、了解过一些传统的消息中间件（RabbitMQ, ActiveMQ, ZeroMQ, Tibco EMS/FTL, IBM MQ/LLM以及我们自研的消息中间件）。最近的工作则一直是基于Kafka展开的。Kafka的很多设计和理念和传统的消息中间件不太一样，谈谈自己的浅薄认识。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kafka" scheme="http://lday.me/tags/Kafka/"/>
    
  </entry>
  
</feed>
